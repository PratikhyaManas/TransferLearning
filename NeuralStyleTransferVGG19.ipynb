{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralStyleTransferVGG19.ipynb",
      "provenance": [],
      "mount_file_id": "1dg5XrWkC4Gb0m9CpCulX-JCAqRaZx7b3",
      "authorship_tag": "ABX9TyOjlhe2SxzJWzYmiq65XBvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PratikhyaManas/TransferLearning/blob/master/NeuralStyleTransferVGG19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWlIU3-IIRkA",
        "colab_type": "text"
      },
      "source": [
        "# **Neural Style Transfer Learning using VGG19 Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUavIleLIlh6",
        "colab_type": "text"
      },
      "source": [
        "# Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46gcPUt0HFgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMAgpGwGIs6c",
        "colab_type": "text"
      },
      "source": [
        "# Mounting the Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCvwIKkLIMKi",
        "colab_type": "code",
        "outputId": "a36da18b-e2b2-4ce0-97bc-72a5d3460748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VuSb1seIx-I",
        "colab_type": "text"
      },
      "source": [
        "# Check the Availablity of GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gRMWFhcIb8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set GPU or CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFfgjVqOI7Go",
        "colab_type": "text"
      },
      "source": [
        "# Image Transformation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6TX_xn0InbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set output image size\n",
        "imsize = 256\n",
        "\n",
        "loader = transforms.Compose([\n",
        "    transforms.Resize(imsize), # Resize images\n",
        "    transforms.ToTensor()]) # Convert to torch tensor\n",
        "\n",
        "def image_loader(image_name):\n",
        "    image = Image.open(image_name)\n",
        "    image = image.resize((imsize,imsize)) # Resize\n",
        "    image = loader(image).unsqueeze(0)\n",
        "    return image.to(device, torch.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az9RWRMCJBs7",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Loss Function of Content Images, Style Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOZqm1sNIrVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Loss Functions\n",
        "\n",
        "# Content loss\n",
        "class ContentLoss(nn.Module):\n",
        "    def __init__(self, target):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.target = target.detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.loss = F.mse_loss(input, self.target)\n",
        "        return input\n",
        "\n",
        "# Style loss\n",
        "def gram_matrix(input):\n",
        "    a, b, c, d = input.size()\n",
        "    # a: batch size(=1)\n",
        "    # b: number of feature maps\n",
        "    # (c,d): dimensions of a f. map (N=c*d)\n",
        "\n",
        "    features = input.view(a*b, c*d) # Reshape to transpose F_xl\n",
        "\n",
        "    # Matrix multiplication to compute the gram product\n",
        "    G = torch.mm(features, features.t())\n",
        "\n",
        "    # Normalize gram matrix values by dividing by # elements in each feature map\n",
        "    # Important bc style features tend to be in deeper network layers\n",
        "    G_norm = G.div(a*b*c*d)\n",
        "\n",
        "    return G_norm\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    def __init__(self, target_feature):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.target = gram_matrix(target_feature).detach()\n",
        "\n",
        "    def forward(self, input):\n",
        "        G = gram_matrix(input)\n",
        "        self.loss = F.mse_loss(G, self.target)\n",
        "        return input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J63R5x3JMuh",
        "colab_type": "text"
      },
      "source": [
        "# Building the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMaX9b4WI1no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Normalize input image w/ module so we can put it in a nn.Sequential layer\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        \"\"\"\n",
        "        Reshape mean and std into [C x 1 x 1] to work w/ image Tensor of shape\n",
        "        [B x C x H x W]\n",
        "        B: batch size\n",
        "        C: # of channels\n",
        "        H: height\n",
        "        W: width\n",
        "        \"\"\"\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, img):\n",
        "        # Normalize image\n",
        "        img_norm = (img - self.mean) / self.std\n",
        "        return img_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSVts-9PI5v-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Network Architecture\n",
        "# Set which layers we want to compute style and content losses at\n",
        "content_layers_default = ['conv_4']\n",
        "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
        "\n",
        "def create_network_with_losses(cnn, norm_mean, norm_std,\n",
        "        style_img, content_img,\n",
        "        content_layers=content_layers_default,\n",
        "        style_layers=style_layers_default):\n",
        "    \"\"\"\n",
        "    Sequential module contains ordered list of child modules, in order of depth.\n",
        "    e.g. vgg19.features contains (Conv2d, ReLU, MaxPool2d, Conv2d, ReLU...)\n",
        "\n",
        "    Want to add content and style loss layers after the convolution layer they\n",
        "    are detecting by creating new Sequential module w/ content loss and style loss\n",
        "    modules correctly inserted.\n",
        "    \"\"\"\n",
        "    print('Creating network...')\n",
        "\n",
        "    cnn = copy.deepcopy(cnn)\n",
        "\n",
        "    # Normalize\n",
        "    normalization = Normalization(norm_mean, norm_std).to(device)\n",
        "\n",
        "    # Keep track of losses\n",
        "    content_losses = []\n",
        "    style_losses = []\n",
        "\n",
        "    # Assuming cnn is nn.Sequential, make new Sequential layer to add\n",
        "    model = nn.Sequential(normalization)\n",
        "\n",
        "    i = 0 # Increment for each convolutional layer\n",
        "    for layer in cnn.children():\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            # 2D convolutional layer\n",
        "            i += 1\n",
        "            name = 'conv_{}'.format(i)\n",
        "        elif isinstance(layer, nn.ReLU):\n",
        "            # rectified linear unit layer\n",
        "            name = 'relu_{}'.format(i)\n",
        "            layer = nn.ReLU(inplace=False) # Replace w/ out-of-place ReLU\n",
        "        elif isinstance(layer, nn.MaxPool2d):\n",
        "            # 2d max pooling layer\n",
        "            name = 'pool_{}'.format(i)\n",
        "        elif isinstance(layer, nn.BatchNorm2d):\n",
        "            # batch normalization layer\n",
        "            name = 'bnorm_{}'.format(i)\n",
        "        else:\n",
        "            layer_name = layer.__class__.__name__\n",
        "            raise RuntimeError('Unrecognized layer: {}'.format(layer_name))\n",
        "\n",
        "        model.add_module(name, layer)\n",
        "\n",
        "        # Add content loss layer to network if current layer is a content layer\n",
        "        if name in content_layers:\n",
        "            target = model(content_img).detach()\n",
        "            content_loss = ContentLoss(target)\n",
        "            model.add_module('content_loss_{}'.format(i), content_loss)\n",
        "            content_losses.append(content_loss)\n",
        "\n",
        "        # Add style loss layer if current layer is a style layer\n",
        "        if name in style_layers:\n",
        "            target_feature = model(style_img).detach()\n",
        "            style_loss = StyleLoss(target_feature)\n",
        "            model.add_module('style_loss_{}'.format(i), style_loss)\n",
        "            style_losses.append(style_loss)\n",
        "\n",
        "\n",
        "    # Trim off layers after the last content and style losses\n",
        "    for i in range(len(model)-1, -1, -1):\n",
        "        current_layer = model[i]\n",
        "        if isinstance(current_layer, ContentLoss) or isinstance(current_layer, StyleLoss):\n",
        "            break\n",
        "\n",
        "    model = model[:(i+1)]\n",
        "\n",
        "    return model, style_losses, content_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdPrN0QlJbXb",
        "colab_type": "text"
      },
      "source": [
        "# Defining the Optimizer Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UXAfej8JA3l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Gradient Descent\n",
        "def get_input_optimizer(input_image):\n",
        "    \"\"\"\n",
        "    Set optimizer to use the Limited-memory BFGS optimization algorithm\n",
        "    \"\"\"\n",
        "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "### Neural Style Transfer\n",
        "def run_style_transfer(cnn, norm_mean, norm_std,\n",
        "        content_img, style_img, input_img,\n",
        "        num_steps=300, style_weight=1000000, content_weight=1):\n",
        "    \"\"\" Run the style transfer. \"\"\"\n",
        "    print('Building style transfer model...')\n",
        "    model, style_losses, content_losses = create_network_with_losses(cnn, norm_mean, norm_std, style_img, content_img)\n",
        "    optimizer = get_input_optimizer(input_img)\n",
        "\n",
        "    run = [0]\n",
        "\n",
        "    while run[0] <= num_steps:\n",
        "        def closure():\n",
        "            # Correct values of updated input image by clamping\n",
        "            input_img.data.clamp_(0,1)\n",
        "\n",
        "            optimizer.zero_grad() # Reset gradients to zero for backward pass\n",
        "            model(input_img)\n",
        "            style_score = 0\n",
        "            content_score = 0\n",
        "\n",
        "            for sl in style_losses:\n",
        "                style_score += sl.loss\n",
        "            for cl in content_losses:\n",
        "                content_score += cl.loss\n",
        "\n",
        "            # Weight style and content scores\n",
        "            style_score *= style_weight\n",
        "            content_score *= content_weight\n",
        "\n",
        "            loss = style_score + content_score\n",
        "            loss.backward()\n",
        "\n",
        "            run[0] += 1\n",
        "            if run[0] % 50 == 0:\n",
        "                print('Run {}:'.format(run))\n",
        "                print('Style loss: {:4f}'.format(style_score.item()))\n",
        "                print('Content loss: {:4f}'.format(content_score.item()))\n",
        "                print()\n",
        "\n",
        "            return style_score + content_score\n",
        "\n",
        "        optimizer.step(closure)\n",
        "\n",
        "        # Clamp one last time\n",
        "        input_img.data.clamp_(0,1)\n",
        "\n",
        "        return input_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkcUOleQJmvh",
        "colab_type": "text"
      },
      "source": [
        "# Running the Pre Trained VGG19 Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHOcM3yYJGP-",
        "colab_type": "code",
        "outputId": "5030a36b-b916-463f-fdb3-cbabd2e81936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        }
      },
      "source": [
        "### Run algorithm\n",
        "# Import model\n",
        "\"\"\"\n",
        "Use pretrained, 19-layer VGG network.\n",
        "Use features module and set to evaluation mode.\n",
        "\"\"\"\n",
        "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
        "\n",
        "# Normalize images w/ mean and std\n",
        "cnn_norm_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
        "cnn_norm_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
        "\n",
        "# Set style image, content folder, and output folder paths\n",
        "style_path = '/content/drive/My Drive/ML_Datasets/Neural_Transfer/style_images/scooby3.jpg'\n",
        "content_paths = '/content/drive/My Drive/ML_Datasets/Neural_Transfer/Pratikh/*'\n",
        "output_folder = '/content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/'\n",
        "\n",
        "style_img = image_loader(style_path)\n",
        "\n",
        "count = 0\n",
        "for content_path in glob.glob(content_paths):\n",
        "\n",
        "    # Set content and result paths\n",
        "    output_path = output_folder + 'result_{}.jpg'.format(count)\n",
        "    content_img = image_loader(content_path)\n",
        "\n",
        "    # Set input image: can use white noise, or a copy of the input image\n",
        "    input_img = content_img.clone() # Input image copy\n",
        "\n",
        "     # Check style and content images are same size\n",
        "    assert style_img.size() == content_img.size(), 'Style and content images need to be the same size.'\n",
        "\n",
        "    ### Run style transfer\n",
        "    output = run_style_transfer(cnn, cnn_norm_mean, cnn_norm_std, content_img, style_img, input_img)\n",
        "\n",
        "    # Write output image to file\n",
        "    print('Writing output to file: ', output_path)\n",
        "    torchvision.utils.save_image(output, output_path)\n",
        "\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building style transfer model...\n",
            "Creating network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  if sys.path[0] == '':\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_0.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_1.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_2.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_3.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_4.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_5.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_6.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_7.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_8.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_9.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_10.jpg\n",
            "Building style transfer model...\n",
            "Creating network...\n",
            "Writing output to file:  /content/drive/My Drive/ML_Datasets/Neural_Transfer/output/Pratikh7/result_11.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OdUByy0J31j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}